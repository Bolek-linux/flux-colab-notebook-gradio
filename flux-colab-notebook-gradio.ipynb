{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uruchomienie FLUX z Interfejsem Gradio (Wersja Poprawiona i Stabilna)\n",
    "\n",
    "Ten notatnik uruchamia model generowania obrazÃ³w FLUX z wygodnym interfejsem graficznym Gradio. ZostaÅ‚ on w peÅ‚ni poprawiony, aby dziaÅ‚aÅ‚ w aktualnym Å›rodowisku Google Colab, omijajÄ…c wszystkie problemy z zaleÅ¼noÅ›ciami.\n",
    "\n",
    "## KROK 0: Konfiguracja Åšrodowiska Wykonawczego\n",
    "\n",
    "Zanim cokolwiek uruchomisz, upewnij siÄ™, Å¼e uÅ¼ywasz akceleratora GPU.\n",
    "\n",
    "1.  W menu na gÃ³rze wybierz: **Åšrodowisko wykonawcze (Runtime)**.\n",
    "2.  NastÄ™pnie kliknij: **ZmieÅ„ typ Å›rodowiska wykonawczego (Change runtime type)**.\n",
    "3.  Z listy rozwijanej \"Akcelerator sprzÄ™towy\" wybierz **T4 GPU** i kliknij Zapisz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 1: Instalacja ZaleÅ¼noÅ›ci (Wymaga Restartu!)\n",
    "\n",
    "PoniÅ¼sza komÃ³rka zainstaluje wszystkie potrzebne biblioteki w starych, ale w peÅ‚ni kompatybilnych ze sobÄ… wersjach, tworzÄ…c \"kapsuÅ‚Ä™ czasu\" dla naszego skryptu.\n",
    "\n",
    "**INSTRUKCJA:**\n",
    "1.  Uruchom poniÅ¼szÄ… komÃ³rkÄ™.\n",
    "2.  **Cierpliwie poczekaj**, aÅ¼ zakoÅ„czy swoje dziaÅ‚anie (moÅ¼e to potrwaÄ‡ kilka minut).\n",
    "3.  Po zakoÅ„czeniu, **MUSISZ zrestartowaÄ‡ sesjÄ™**. W menu wybierz: **Åšrodowisko wykonawcze (Runtime) -> Zrestartuj sesjÄ™ (Restart session)** lub uÅ¼yj skrÃ³tu **Ctrl+M .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÃ“RKA 1: INSTALACJA\n",
    "\n",
    "# Krok A: Naprawa problemu z NumPy\n",
    "!pip install \"numpy<2\"\n",
    "\n",
    "# Krok B: Instalacja historycznie poprawnych wersji bibliotek\n",
    "!pip install torch==2.1.0 torchvision==0.16.0 xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Krok C: Instalacja pozostaÅ‚ych zaleÅ¼noÅ›ci\n",
    "!pip install -q torchsde einops diffusers accelerate gradio==3.50.2 python-multipart==0.0.12\n",
    "\n",
    "print(\"\\nâœ… ZakoÅ„czono instalacjÄ™. Teraz najwaÅ¼niejszy krok:\")\n",
    "print(\"ðŸ‘‰ ZRESTARTUJ SESJÄ˜ (Runtime -> Restart session lub Ctrl+M .) ðŸ‘ˆ\")\n",
    "print(\"Po restarcie przejdÅº do komÃ³rki w KROKU 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KROK 2: Uruchomienie Aplikacji (Po Restarcie!)\n",
    "\n",
    "TÄ™ komÃ³rkÄ™ uruchom **DOPIERO PO** poprawnym wykonaniu KROKU 1 i zrestartowaniu sesji.\n",
    "\n",
    "Ten skrypt pobierze modele i uruchomi interfejs Gradio. Po chwili na dole pojawi siÄ™ publiczny link, ktÃ³ry naleÅ¼y otworzyÄ‡ w nowej karcie przeglÄ…darki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÃ“RKA 2: URUCHOMIENIE\n",
    "\n",
    "# Upewniamy siÄ™, Å¼e jesteÅ›my w dobrym miejscu\n",
    "%cd /content\n",
    "\n",
    "# Klonowanie repozytorium (zwrÃ³Ä‡ uwagÄ™ na nowÄ… gaÅ‚Ä…Åº 'totoro4')\n",
    "!git clone -b totoro4 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
    "%cd /content/TotoroUI\n",
    "!apt -y install -qq aria2\n",
    "\n",
    "# Pobieranie nowych modeli (all-in-one i LoRA)\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8-all-in-one.safetensors -d /content/TotoroUI/models/checkpoints -o flux1-dev-fp8-all-in-one.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux_realism_lora.safetensors -d /content/TotoroUI/models/loras -o flux_realism_lora.safetensors\n",
    "\n",
    "# Reszta kodu z notatnika (importy, funkcje, interfejs Gradio)\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('/content/TotoroUI') # WaÅ¼ne, by Python znalazÅ‚ moduÅ‚y\n",
    "\n",
    "import nodes\n",
    "from nodes import NODE_CLASS_MAPPINGS\n",
    "from totoro_extras import nodes_custom_sampler\n",
    "from totoro_extras import nodes_flux\n",
    "from totoro import model_management\n",
    "import gradio as gr\n",
    "\n",
    "CheckpointLoaderSimple = NODE_CLASS_MAPPINGS[\"CheckpointLoaderSimple\"]()\n",
    "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
    "FluxGuidance = nodes_flux.NODE_CLASS_MAPPINGS[\"FluxGuidance\"]()\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
    "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet, clip, vae = CheckpointLoaderSimple.load_checkpoint(\"flux1-dev-fp8-all-in-one.safetensors\")\n",
    "\n",
    "def closestNumber(n, m):\n",
    "    q = int(n / m)\n",
    "    n1 = m * q\n",
    "    if (n * m) > 0:\n",
    "        n2 = m * (q + 1)\n",
    "    else:\n",
    "        n2 = m * (q - 1)\n",
    "    if abs(n - n1) < abs(n - n2):\n",
    "        return n1\n",
    "    return n2\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip):\n",
    "    global unet, clip\n",
    "    if seed == 0:\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    print(f\"UÅ¼yte ziarno (seed): {seed}\")\n",
    "    unet_lora, clip_lora = LoraLoader.load_lora(unet, clip, \"flux_realism_lora.safetensors\", lora_strength_model, lora_strength_clip)\n",
    "    cond, pooled = clip_lora.encode_from_tokens(clip_lora.tokenize(positive_prompt), return_pooled=True)\n",
    "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
    "    cond = FluxGuidance.append(cond, guidance)[0]\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    guider = BasicGuider.get_guider(unet_lora, cond)[0]\n",
    "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
    "    sigmas = BasicScheduler.get_sigmas(unet_lora, scheduler, steps, 1.0)[0]\n",
    "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
    "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
    "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
    "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(\"/content/flux.png\")\n",
    "    return \"/content/flux.png\"\n",
    "\n",
    "with gr.Blocks(analytics_enabled=False) as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            positive_prompt = gr.Textbox(lines=3, interactive=True, value=\"cute anime girl with massive fluffy fennec ears and a big fluffy tail blonde messy long hair blue eyes wearing a maid outfit with a long black dress with a gold leaf pattern and a white apron eating a slice of an apple pie in the kitchen of an old dark victorian mansion with a bright window and very expensive stuff everywhere\", label=\"Prompt\")\n",
    "            width = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"width\")\n",
    "            height = gr.Slider(minimum=256, maximum=2048, value=1024, step=16, label=\"height\")\n",
    "            seed = gr.Slider(minimum=0, maximum=18446744073709551615, value=0, step=1, label=\"seed (0=random)\")\n",
    "            steps = gr.Slider(minimum=4, maximum=50, value=20, step=1, label=\"steps\")\n",
    "            guidance = gr.Slider(minimum=0, maximum=20, value=3.5, step=0.5, label=\"guidance\")\n",
    "            lora_strength_model = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.1, label=\"lora_strength_model\")\n",
    "            lora_strength_clip = gr.Slider(minimum=0, maximum=1, value=1.0, step=0.1, label=\"lora_strength_clip\")\n",
    "            sampler_name = gr.Dropdown([\"euler\", \"heun\", \"heunpp2\", \"heunpp2\", \"dpm_2\", \"lms\", \"dpmpp_2m\", \"ipndm\", \"deis\", \"ddim\", \"uni_pc\", \"uni_pc_bh2\"], label=\"sampler_name\", value=\"euler\")\n",
    "            scheduler = gr.Dropdown([\"normal\", \"sgm_uniform\", \"simple\", \"ddim_uniform\"], label=\"scheduler\", value=\"simple\")\n",
    "            generate_button = gr.Button(\"Generate\")\n",
    "        with gr.Column():\n",
    "            output_image = gr.Image(label=\"Generated image\", interactive=False)\n",
    "\n",
    "    generate_button.click(fn=generate, inputs=[positive_prompt, width, height, seed, steps, sampler_name, scheduler, guidance, lora_strength_model, lora_strength_clip], outputs=output_image)\n",
    "\n",
    "print(\"\\nUruchamiam interfejs Gradio...\")\n",
    "demo.queue().launch(inline=False, share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
